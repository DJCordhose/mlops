<!doctype html>
<html>

<head>
	<meta charset="utf-8">
	<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=10.0, user-scalable=yes">

	<title>Workshop: MLOps M3 2023</title>

	<link rel="stylesheet" href="revealjs/reveal.js/dist/reset.css" />
    <link rel="stylesheet" href="revealjs/reveal.js/dist/reveal.css" />
    <link rel="stylesheet" href="revealjs/reveal.js/dist/theme/white.css" />

    <!-- Theme used for syntax highlighted code -->
	<!-- <link rel="stylesheet" href="revealjs/reveal.js/plugin/highlight/monokai.css"> -->
	<!-- <link rel="stylesheet" href="revealjs/reveal.js/plugin/highlight/zenburn.css"> -->
    <link rel="stylesheet" href="revealjs/highlight-js-github-theme.css" />
    <link rel="stylesheet" href="revealjs/styles.css" />

</head>

<body style="background-color: whitesmoke;">
	<div class="reveal">
		<div class="slides">

<!-- 
M3 Workshop

M3

Titel: MLOps mit TensorFlow, Docker und Kubernetes 
Untertitel: Wie bringt man ein Machine Learning Modell in Produktion und hält es dort?

Nur Titel: MLOps - wie bringt man ein Machine Learning Modell in Produktion und hält es dort?

Level: Fortgeschritten

MLOps beschäftigt sich mit dem Thema, ein Machine Learning Modell in Produktion zu bringen und es dort erfolgreich zu betreiben.

Dementsprechend besteht der Workshop aus drei Teilen:  
1. Professionalisierung: Umwandlung eines bestehenden Notebooks in Libraries, Tests, Scripte und ein resilientes Modell
2. Produktivsetzung: Erstellen eines OpenAPI Server, Betrieb mit Docker und Kubernetes  
3. Monitoring: Drift Detection mit Prometheus, Evidently und Grafana 

Dieser Workshop hat den Schwerpunkt auf den späteren Phasen eines Machine Learning Projekts, also der Professionalisierung und dem Betrieb. 
Er schließt damit an den MLOps Workshop der letztjährigen M3 an. 

Vorkenntnisse:
Als ML Werkzeug werden wir TensorFlow nutzen, allerdings ist dies nur ein Beispiel und alles gezeigte gilt auch für alle
anderen Frameworks. Es wird lediglich Erfahrung mit Python und ein Verständnis für ein Machine Learning Projekt
vorausgesetzt. Alles andere wird - soweit nötig - im Workshop eingeführt.


Lernziel:
Teilnehmer sollen anhand eines in sich stimmigen Satzes von Werkzeugen die Konzepte und Ansätze im Bereich MLOps kennen lernen und diskutieren können.
-->

<!-- <section data-markdown class="preparation">
	<textarea data-template>
### Vorbereitung

</textarea>
</section>
` -->


<!-- <section data-markdown style="font-size: medium;">
	<textarea data-template>
## Agenda - Lang

### Block I - Einleitung / Grundlagen (Olli)
1. Unser Beispiel
1. Unser Ausgangspunkt: Notebooks
1. Übersicht MLOps

### Block II - Professionalisierung (Yannick)
1. Container mit Docker
1. Umwandlung des bestehenden Notebooks in Libraries, Tests und Scripte
1. Erstellen eines OpenAPI Server

### Block III - Betrieb (Tobi)
1. Skalierung mit Docker und Kubernetes
1. Betrieb des Servers
1. Die Trainings-Pipeline
1. Deployment eines neuen Servers und Modells

### Block IV - Monitoring (Olli)
1. Drift verstehen und detektieren
1. Drift Detection mit Prometheus, Evidently und Grafana

### Abschluss (Alle)
1. Zusammenfassung
1. Offene Fragen
1. Ausblick
	</textarea>
</section> -->

<!-- <section data-markdown>
	<textarea data-template>
## Zeitplan

ab 09:00 Uhr: Wer noch Hilfe bei der Installation braucht, kann bereits jetzt erscheinen

10:00 Uhr: Beginn

Einleitung / Grundlagen (Olli 1h)
- Unser Beispiel
- Unser Ausgangspunkt: Notebooks
- Übersicht MLOps

Professionalisierung (Yannick 1,5h)
- Bestehendes Notebook professionalisieren 
- Das Modell mit FastAPI via REST bereitstellen
- Einführung in die Containerisierung
- Training und Deployment in Containern

Ausblick auf den Block nach der Mittagspause
- Mit vorhandener Infrastructure as Code einen Cluster erzeugen

12:30 - 13:30 Uhr: Mittagspause

Betrieb (Tobi 1,5h)
- Aufsetzen eines Clusters mittels Terraform in Kind
- Überblick über die verschiedenen Anwendungen im Cluster
- Die Build und Trainings-Pipeline ausführen (CI)
- Deployment der ML Anwendung und des Modells (CD)

15:00 - 15:15 Uhr: Kaffeepause

Monitoring (Olli 1h)
- Drift verstehen und detektieren
- Drift Detection mit Prometheus, Evidently und Grafana

16:15 - 16:30 Uhr: Kaffeepause

Abschluss (Alle 0,5h)
- Zusammenfassung
- Offene Fragen
- Ausblick

ca. 17:00 Uhr: Ende

Notwendige Software
* Einen aktuellen Chrome Browser und ein Google Login
* git: https://git-scm.com/downloads
* Empfohlen für Windows: WSL2 mit Ubuntu für Shell und Docker: https://ubuntu.com/tutorials/install-ubuntu-on-wsl2-on-windows-11-with-gui-support
* kubectl: https://kubernetes.io/de/docs/tasks/tools/install-kubectl/
* docker: https://docs.docker.com/get-docker/`
* kind: https://kind.sigs.k8s.io/docs/user/quick-start#installation
* terraform: https://developer.hashicorp.com/terraform/downloads?product_intent=terraform
* Als Bonus: k9s: https://k9scli.io/topics/install/
	</textarea>
</section> -->


<!-- <section data-markdown class="hide" style="font-size: xx-small">
	<textarea data-template>
### Story

1. Problemstellung: innovative Kfz-Versicherungsgesellschaft
1. Ausgangspunkt mit Colab: notebooks/exploration.ipynb
   1. Features
   1. Was wollen wir vorhersagen?
   1. Tests, Training, 
1. MLOps Grundlagen / Überblick über die Phasen, wieso kann ich damit nicht in Prod gehen
1. Professionalisierung
   1. Intro Docker
   1. Notebook in Libs und Scripte
1. Build: Scripte im Docker Container laufen lassen
   1. `~/scripts$ ./train.py -d ../data/reference.csv -m classifier`
   1. `~/scripts$ ./validate.py -d ../data/reference.csv -m classifier`
   1. Modell in model Ordner schieben
1. Server
   1. `(mlops-workshop-d2d) olli@DESKTOP-BEN73DP:~/mlops-data2day/app$ ./app.py`
   1. `http://localhost:8080/ping`
   1. `(mlops-workshop-d2d) olli@DESKTOP-BEN73DP:~/mlops-data2day$ python -m http.server`
   1. `http://localhost:8000/app/client.html`
   1. in `app/client.html` Werte anpassen und herumspielen damit
1. Produktion in Docker
   1. Notwendig: Volumes von Prometheus und Grafana platt machen, um von 0 zu starten???
   1. `(base) olli@DESKTOP-BEN73DP:~/mlops-data2day$ docker compose up --build`
   1. `http://localhost:8085/metrics`
   1. `http://localhost:9090`
   1. `http://localhost:3000/`
1. Produktion simulieren
   1. Story:
      1. die Performance des Modells degradiert
	  1. aber wir haben erst nach Jahren eine Ground Truth, die uns das anhand der Metrik zeigt
	  1. wir simulieren 3 Jahre Betrieb mit
         1. Leute werden immer Älter, das passiert aber langsam (age)
	     1. Es wird immer weniger Auto gefahren, Leute steigen um auf die Bahn und öffentliche Verkehrsmittel (miles)
	     1. Die Sicherheit der Autos wird immer besser und der Einfluss der individuellen Fahrleistung wird verringert (emergency_braking, pred) 
   1. `(mlops-workshop-d2d) olli@DESKTOP-BEN73DP:~/mlops-data2day$ ./scripts/example_run_request.py` 
   1. `http://localhost:8085/metrics`
   1. `http://localhost:3000/d/U54hsxv7k/evidently-data-drift-dashboard?orgId=1&refresh=5s`
</textarea>
</section> -->


<section data-markdown class="preparation">
# Nur für uns Vortragenden  
</section>

<section data-markdown class="preparation">
### Didaktische Prinzipien

- Phasen klar haben, und klar kommunizieren, z.B. Übung: 
  - was sollen die Leute machen
  - ist die Aufgabenstellung klar?
  - wie viel Zeit
  - wann sollen die loslegen 
- sich nicht in Details verlieren, 
  - den Überblick behalten
  - es muss nicht jeder alles immer verstehen oder am laufen haben
  - versuchen, das Ziel im der Übung/Blocks im Auge zu behalten
- dennoch: Störung haben Vorrang 
  - Unklarheiten
  - Zweifel
  - Fragen / Diskussionen
</section>

<section data-markdown style="font-size: medium;" class="preparation">
	<textarea data-template>
# Blöcke

<div class="container">
  <div class="col">


## I Einleitung / Grundlagen
#### Was sollen die Teilnehmer lernen?
- Abholen mit Notebook: So sieht vielleicht ein Projekt aus von dem die Teilnehmer denken, es sei fertig
- Bei uns geht es jetzt erst los
- Warum MLOPs?

### Übung
- Vertraut machen mit dem Anwendungensfall und dem Notebook, alle auf denselben Stand bringen

## II Professionalisierung
### Was sollen die Teilnehmer lernen?
- Was ist der Unterschied zwischen einem Notebook und einem Software Projekt
- Man bringt kein Notebook in Produktion
- Ein ML Service ist weit mehr als nur ein Modell

### Übung
1. Image über Dockerfile bauen und starten
1. Über Swagger einen Request gegen den Server machen
</div>
<div class="col">

## III Betrieb
### Was sollen die Teilnehmer lernen?
- Einen Service manuell in Produktion zu bringen, ist wenig professionell
- Änderungen müssen automatisch gebaut und deployed werden

### Übung
1. Installation mit Terraform (wenn nicht schon am Anfang gemacht)
   * wie in der Readme beschrieben
1. Pipeline und Trigger erkunden
   1. Kapazität des Modells herunterschrauben
   1. Pipeline läuft nicht durch, da Qualität des Modells zu niedrig
   1. Kapazität des Modells wider heraufschrauben und Pipeline läuft wieder durch
1. Modell-Code in Gitea erweitern, sodass es genug Kapazität hat bis es erfolgreich durch die Pipeline geht 

## IV Monitoring
### Was sollen die Teilnehmer lernen?
- Man ist nicht fertig, wenn das Modell ist Produktion ist
- Monitoring von ML ist besonders, braucht Statistik und Interpretation

### Übung
1. Requests abfeuern
1. Drift beobachten
</div>
</div>
</textarea>
</section>


<section data-markdown style="font-size: xx-large;">
	<textarea data-template>
### Bevor es los geht

1. WLAN: IHK_Veranstaltung (steht auf der Rückseite deines Batches)
1. Diese Folien: https://bit.ly/m3-2023-mlops
1. Für Windows WSL2 Shell mit Ubuntu installieren
   * Details in der Installationsanleitung
   * bei Problemen auf nächste Folie gucken
1. Sicher stellen, dass alle notwendige Software installiert ist: https://www.m3-konferenz.de/veranstaltung-20093-0-mlops--wie-bringt-man-ein-ml-modell-in-produktion-und-haelt-es-dort.html
1. Projekt klonen
    * https://github.com/openknowledge/mlops-m3
    * `git clone git@github.com:openknowledge/mlops-m3.git`

_Bei Problemen helfen Yannick, Tobias, und Olli_
</textarea>
</section>


<section data-markdown>
  <textarea data-template>
### WSL2 auf Windows (Home)

Docker auf Windows Home braucht WLS2

<img src="img/Windows-features-wsl.png">

https://kind.sigs.k8s.io/docs/user/using-wsl2/
  </textarea>
</section>



			<section data-markdown>
				<textarea data-template>
# MLOps
## Wie bringt man ein ML-Modell in Produktion und hält es dort?

M3 2023, https://www.m3-konferenz.de/veranstaltung-20093-0-mlops--wie-bringt-man-ein-ml-modell-in-produktion-und-haelt-es-dort.html

_Yannick Habecker, Tobias Kurzydym, Oliver Zeigermann_

Folien: https://bit.ly/m3-2023-mlops
    </textarea>
			</section>

<section data-markdown>
  <textarea data-template>
### Wer ist Yannick

<img src='img/yannick.jpg'>

<p>
<a target="_blank" href="mailto:yannick.habecker@openknowledge.de">Yannick Habecker</a>:
Dev@OpenKnowledge
</p>    
</textarea>
</section>

<section data-markdown>
  <textarea data-template>
### Wer ist Tobias

<img src='img/tobias_kurzydym.png'>

<p>
<a target="_blank" href="mailto:tobias.kurzydym@openknowledge.de">Tobias Kurzydym</a>:
Dev@OpenKnowledge
</p>    
</textarea>
</section>


<section data-markdown>
  <textarea data-template>
### Wer ist Olli

<img src='img/olli-opa.jpeg'>
<p>
<a target="_blank" href="mailto:oliver@zeigermann.de">Oliver Zeigermann</a>:
ML Architekt
</p>    
</textarea>
</section>

<section data-markdown>
	<textarea data-template>
### Wer seid ihr?

* Was macht ihr?
* Was wisst ihr schon?
* Warum seid ihr hier?
</textarea>
</section>

<section data-markdown>
	<textarea data-template>
## Agenda

1. Einleitung / Grundlagen
2. Professionalisierung
3. Betrieb
4. Monitoring
5. Abschluss
	</textarea>
</section>

<section data-markdown>
	<textarea data-template>
### Handout

<a href="https://www.embarc.de/architektur-spicker/11-container-anwendungen/">
<img src="img/Spicker_11_Container-Anwendungen_Vorschaubild3er.png">
</a>

https://www.embarc.de/architektur-spicker/11-container-anwendungen
</textarea>
</section>

<section data-markdown>
	<textarea data-template>
## Agenda

1. *Einleitung / Grundlagen*
2. Professionalisierung
3. Betrieb
4. Monitoring
5. Abschluss
	</textarea>
</section>

<section data-markdown>
	<textarea data-template>

## Unser Fallbeispiel und Datenbasis

</textarea>
</section>

<section data-markdown>
	<textarea data-template>
### Unser Beispiel: Vorhersage von Risiken

* Wir sind CTO einer hochinnovativen Kfz-Versicherungsgesellschaft
* Anders als andere Versicherungsgesellschaften bestimmen wir den Tarif anhand der geschätzen Anzahl von Unfällen pro Kunde
* Zielsetzung: Wie viele Unfälle werden die potenziellen Kunden haben?

<img src='img/pixabay/accident-151668_1280.png' style="height: 230px">
</textarea>
</section>

<section data-markdown>
  <textarea data-template>
### Features

<img src="img/features.jpg">
  </textarea>
</section>  

<!-- <section data-markdown>
### Features
* Fahrer
  * *training*: 0/1, hat der Fahrer ein Fahrertraining absolviert
  *	*age*: Alter des Fahrers in Jahren
* Fahrzeug  
  *	*emergency_braking*: 0/1, hat das Fahrzeug ein Notbremssystem 
  *	*braking_distance*: Bremsweg aus Tempo 100
  * *power*: Leistung in KW
*	*miles*: Jährliche Fahrleistung in Meilen
* Das wollen wir vorhersagen
  *	*risk*: Unfallrisiko, nach unten und nach oben offen, kann negativ sein
  *	*group* / *group_name*:	Einteilung in Gruppe anhand des Risikos
</section> -->

<section data-markdown>
	<textarea data-template>
## Daten entstehen über die Zeit

Die Ground Truth (GT) kommt verzögert und kann sich über die Zeit noch verändern

<img src="img/ground-truth-zeitstrahl.png">    

</textarea>
</section>

<section data-markdown style="font-size: xx-large;" class="hands-on">
  <textarea data-template>
## Hands-On 1 - Unser Ausgangspunkt

* Neuronales Netzwerk mit TensorFlow
* 3 Hidden Layers, 100 Neuronen pro Layer
* 1500 Datensätze insgesamt
* Training auf allen 6 Features
* Test/Validation auf 300 Datensätzen
* Training auf 1200 Datensätzen
* Normalisierung auf Trainingsdatensätzen
* Accuracy Training/Test > 85%

Notebook auf Colab: https://colab.research.google.com/github/openknowledge/mlops-m3/blob/main/notebooks/exploration.ipynb

</textarea>
</section>  

<section data-markdown>
	<textarea data-template>
## Sind wir jetzt nicht schon fertig?    
  </textarea>
</section>  

<section data-markdown class="fragments">
### Was ist MLOps?

* MLOps ist abgeleitet von DevOps
* Durch MLOps kommt ML in Produktion und wird in Betrieb gehalten
* Dazu kommen eine Reihe von Werkzeugen und Praktiken zum Einsatz
* Überschneidung aus
  * Softwareentwicklung
  * Operations
  * Data Science
</section>

<section data-markdown class="fragments">
### Warum MLOps?

* im akademischen Leben zählt für einen Wettbewerb häufig nur der Score (Güte) des Modells
* dieser Ansatz hat sich im Bereich des Data Science auch in der Praxis breit gemacht
* die Praxis ist aber keine Kaggle Competition
* In-Sample Evaluation (auch wenn wir die vorher abgetrennt haben) sagt nur bedingt etwas für Eignung in
einer praktsichen Anwendung aus
* Out-Of-Sample Evaluation häufig erst im produktiven Betrieb möglich (evtl nur mitlaufen lassen)
</section>

<section data-markdown>
	<textarea data-template>
### Phasen eines ML Projekts

<img src="img/ml-workflow/6.PNG" class="fragment">

</textarea>
</section>

	<section data-markdown class="fragments">
### Phase I: Exploration

das haben wir bisher gemacht

* in der ersten Phase eines Machine Learning Projekts wird die Anwendungsidee validiert und ein
funktionsfähiges Modell entwickelt.
* dabei ist ein schnelles iterieren und ausprobieren von Ideen zentral
* das Ziel ist *nicht* ein sinnvolles Stück Software
* Scripting passt hier besser als Programmieren als Ausdruck für die Tätigkeit
* das Ziel ist eine schnelle Entwicklung
* Phase 1 endet entweder mit
  * einem funktionsfähigen Modell mit dem man in Phase II übergeht oder
  * dem Verwerfen des Ansatzes

	</section>

<section data-markdown>
	<textarea data-template>
### Exploration hinterlässt gern einen Wust an Notebooks

<img src='img/sketch/notebook-explosion-no-title.png'  style="height: 600px;">

</textarea>
</section>

	<section data-markdown class="fragments">
		<textarea data-template>
### Ist ein Wust an Notebooks ein Problem?

* das ist kein Zeichen von einem unprofessionellen Vorgehen
* ergibt sich aus der Arbeitsweise und Zielsetzung
* jeder Experimentator, erprobte ML Ansatz und jede Iteration kann eine neue, komplett entkoppelte Kopie eines Notebooks rechtfertigen
  * "Das Wichtigste in dieser Phase ist die schnelle Iteration" https://twitter.com/marktenenholtz/status/1488134981985583105
  * "Machen Sie ein einfaches Experiment nach dem anderen" https://karpathy.github.io/2019/04/25/recipe/
* natürlich wird dabei teilweise falsch entkoppelt
  * Wir kopieren alles, auch die Teile, die die beiden Notebooks größtenteils unverändert teilen
  * Solange wir aber nicht wissen was die relevant gemeinsamen Teile sind müssen wir damit weiter machen
* welcher Ansatz mehr Liebe verdient wird erst am Ende dieser Phase klar
</textarea>
	</section>

<section data-markdown>
	<textarea data-template>
### Ein ML Modell geht nicht für sich allein in Produktion

<img src="img/ml-system.jpg" style="height: 550px;">

</textarea>
</section>

<section data-markdown class="fragments">
	<textarea data-template>
### Warum Ensemble/Fallback?

* Ein ML Modell ist mit Daten eines bestimmten Bereichs trainiert
  * manche Anfragen sind nicht in diesem Bereich
  * die meisten Modelle haben keine Ahnung wann sie keine Ahnung haben
  * daher müssen wir festlegen, welche Anfrage wohin geleitet wird
* Auch nach der Vorhersage kann entschieden werden, dass die Wahrscheinlichkeit nicht hoch genug ist
  * Dann ein anderes Modell befragen
  * Manche Modelle sind teurer im Betrieb als andere, diese eher später nutzen
* Oft ist ein einfaches heuristisches Regelsystem eine gute Alternative / Fallback
* Bei mehreren passenden Modellen ist auch eine Mehrheitsentscheidung denkbar
* Mehr Vorschläge: https://twitter.com/ChristophMolnar/status/1647881654873063426
  
</textarea>
</section>


<section data-markdown>
	<textarea data-template>
## Agenda

1. Einleitung / Grundlagen
2. *Professionalisierung*
3. Betrieb
4. Monitoring
5. Abschluss
	</textarea>
</section>


	<section data-markdown class="fragments">
### Phase II: Professionalisierung

* in der zweiten Phase wird die skizzierte Lösung in ein langlebiges Projekt umgewandelt
* alle Regeln einer guten Software-Entwicklung gelten von nun an
* Stabilität und Funktionalität wird gewährleistet
* Die Rahmenbedingungen der Produktionsumgebung müssen erfüllt werden
* Art des Deployments, Sprache, Latenz, Speicher, Bandbreite, etc.
* Phase II endet entweder mit
  * reifem Code und Modell mit dem man in Phase III übergeht oder
  * dem Iterieren zurück in Phase I mit neu gewonnenen Erkenntnissen oder falls Rahmenbedingungen nicht erfüllt werden

	</section>


	<section data-markdown>
		<textarea data-template>
<img src='img/sketch/ml-dev-prozess.png' style="height: 650px;">
</textarea>
	</section>

	<section data-markdown class="fragments">
		<textarea data-template>
### Was gehört in Bibliotheken ausgelagert?

* Wir können nicht mit einem Notebook in Produktion gehen
  * also muss alles was wir in Produktion brauchen aus den Notebooks herausgezogen werden
* Bestimmte Teile eines Notebooks haben sich als stabil herausgestellt und sollten nicht bei jeder Kopie entkoppelt werden
* Alles was sich nach Software anfühlt (Klassen, Funktionen, etc.) ist auch Software
  * diese Teile sollten auch wie solche behandelt werden  
* Professionalisierung muss oft gut abhängen
  * Es stellt sich erst langsam heraus, was in ein Modul gehört
  * erste Version der extrahierten Module ist mit Sicherheit nicht endgültig
</textarea>
	</section>

<section data-markdown>
### Scripte	

* Manche Notebooks werden in Phase II zu Skripten, die eine dünne API um die Module sind. 
* Können auch in CI/CD eingebaut werden.

</section>

<section data-markdown>
	<textarea data-template>
### CI/CD bei klassischem DevOps

<img src="img/ci-cd-flow-desktop.png" style="height: 100%">

https://www.redhat.com/en/topics/devops/what-is-ci-cd

	</textarea>
</section>

<section data-markdown>
	<textarea data-template>
### DevOps vs MLOps

<img src="img/devops-vs-mlops.png" style="height: 600px">
</textarea>
</section>

<section data-markdown class="hands-on">
	<textarea data-template>
## Hands-On IIa - wie wollen wir das Modell validieren?

* bei automatisiertem Training können wir ein erfolgreiches Training nicht mehr manuell überwachen
* wie könnten automatisierte Tests auf dem Modell aussehen?
* Diskutiert mit euren Nachbarn und schreibt eure Vorschläge auf
* Inspiration: https://storage.googleapis.com/pub-tools-public-publication-data/pdf/45742.pdf

</textarea>
</section>

<section data-markdown class="hands-on" style="font-size: x-large;">
	<textarea data-template>
## Hands-On IIb - Scripte, manuelles CI/CD

_Wir führen unsere Pipeline aus und sehen uns gemeinsam die Scripte an_

1. Öffne ein Terminal aus deinem Notebook Server heraus (New->Terminal)
1. `cd scripts`
1. Modell trainieren: `./train.py -d ../data/reference.csv -m classifier`
   <!-- * Windows: `python train.py -d ..\data\reference.csv -m classifier` -->
1. Modell validieren: `./validate.py -d ../data/reference.csv -m classifier`
   <!-- * Windows: `python validate.py -d ..\data\reference.csv -m classifier` -->
1. Deploy: Modell in `/model` Ordner schieben

*Aufgabe*
1. Sieh dir die Scripte über die IDE oder den Notebook Server an
1. Welche Validierungen werden verwendet?
1. Passe ggf. die Werte an, sodass alle Scripte durchlaufen
1. Ergänz das Validation Script mit euren Validierungen, wenn sie einfach umzusetzen sind
</textarea>
</section>

<section data-markdown>
	<textarea data-template>
## Agenda

1. Einleitung / Grundlagen
2. Professionalisierung
3. *Betrieb*
4. Monitoring
5. Abschluss
	</textarea>
</section>

<section data-markdown>
	<textarea data-template>
<img src='img/everybody-gansta.png' style="height: 600px">

<small>

https://twitter.com/karpathy/status/1486215976559398915
</small>

</textarea>
</section>


	<section data-markdown class="fragments">
### Phase III: Produktion / Betrieb

in der dritten Phase wird die Lösung in Betrieb genommen

* alle Regeln des produktiven Einsatzes von Software gelten auch hier
* Monitoring hat zusätzliche Herausforderungen
  * Natur und Verteilung der Anfragen und auch Vorhersagen muss permanent überwacht werden
* Phase III endet entweder mit
  * der Abschaltung 
    * entweder bald weil nutzlos oder
    * später weil durch neues System ersetzt
  * dem Iterieren zurück in Phase II mit neu gewonnenen Erkenntnissen
  * dem Iterieren zurück in Phase I mit neu gewonnenen Erkenntnissen oder einem Neuansatz (häufig ebenfalls ein Zeichen für einen Fehlschlag)

			</section>

	<section data-markdown>
		<textarea data-template>
### Nicht aller Code ist für Produktion gedacht		

* Was geht in Produktion
  * Vorhersage
  * Monitoring

* Was geht nicht in Produktion
  * Training
  * Validierungen
  * Analytics
  * Visualisierung
</textarea>
	</section>

<section data-markdown>
  <textarea data-template>
### Was ist Docker?

* Einheitliche, leichtgewichtige Form der Virtualisierung
* Es werden Images gebaut, die die benötigte Software beinhalten und diese werden in einem Container ausgeführt
* Container isolieren die Software von der Umgebung und sind unabhängig von der Infrastruktur
* "docker compose" lässt uns mehrere Container gleichzeitig starten

<img src='img/docker-vm-vergleich.png'  style="height: 250px;">

https://www.docker.com/resources/what-container/

  </textarea>

</section>

<section data-markdown style="font-size: large;" class="hands-on">
	<textarea data-template>
## Hands-On 0 - Docker Installation überprüfen oder herstellen

Helft bitte euren Nachbarn, falls eure Installation schon läuft

1. Wir empfehlen: arbeitet zusammen mit euren direkten Nachbarn
1. Stellt sicher, dass zumindest einer eurer Rechner eine lauffähige Installation hat 
1. Sagt kurz Hallo
1. Installieren (falls noch nicht passiert)
   1. Git : https://git-scm.com/downloads
   1. Docker: https://docs.docker.com/get-docker/
1. Projekt klonen
   * https://github.com/openknowledge/mlops-data2day
   * `git clone git@github.com:openknowledge/mlops-data2day.git`
1. Docker images starten
   * Im Projekt-Verzeichnis: `docker compose up`
   * Wir brauchen die Ports: 8000, 8080, 9090, 8085, 8888, 8889, 9090, 3000
   * Das kann (gerade bei überlastetem Netz) lange dauern
   * sobald das Docker-Kommando abgesetzt ist können wir weiter im Stoff
   * Zwingend brauchen wir die Services erst in Hands-On IIb 
1. Nachdem Docker alle Container gestartet hat überprüfen, dass Jupyter erreichbar ist: http://localhost:8888/   

</textarea>
</section>

<section data-markdown>
	<textarea data-template>
### Übersicht Container

<img src="img/containers-desktop.png">

Das sollte bei dir laufen
</textarea>
</section>

	<section data-markdown class="hands-on">
		<textarea data-template>
## Hands-On IIIa - Produktionsumgebung

1. Im Ordner `app` setzen wir den Rating Service komplett mit Adapter um
1. In `app.py` ist ein einfacher Flask Server implementiert
1. Der Server wird bereits über Docker compose gestartet
1. Checken, ob der Service läuft: http://localhost:8080/ping
1. Über http://localhost:8000/client.html simulieren wir den orchestrierenden/nutzenden Service und machen einfache Requests
1. in `app/client.html` Werte anpassen
   * auf dich als Fahrer
   * außerhalb des gültigen Bereichs des Models 
</textarea>
	</section>

  	<section data-markdown class="hands-on">
		<textarea data-template>
## Hands-On IIIb - Grenzen des Modells

*Diskussion*
* Sollten wir unser Modell für jede Anfrage verwenden?
* Sollten wir alle gültigen Vorhersagen nutzen?

*Code*
1. `app/app.py` und `app/model_prediction.py` enthalten den Server Code
1. Sieh dir an, wie entschieden wird, ob das ML Modell genommen wird
1. Findest du die Bedingungen sinnvoll?
1. Passe sie deinen Vorstellungen an und probiere das Modell wieder über den Client aus
</textarea>
	</section>

<section data-markdown>
	<textarea data-template>
## Agenda

1. Einleitung / Grundlagen
2. Professionalisierung
3. Betrieb
4. *Monitoring*
5. Abschluss
	</textarea>
</section>


<section data-markdown>
  <textarea data-template>
### Machine Learning Anwendungen brauchen Wartung

<img src='img/verfall-1.PNG'>

Das gilt nicht nur für ML Anwendungen, aber bei diesen ist es offensichtlicher
</textarea>
</section>

<section data-markdown>
  <textarea data-template>
### Machine Learning Anwendungen brauchen Wartung

<img src='img/verfall-2.PNG'>

Das gilt nicht nur für ML Anwendungen, aber bei diesen ist es offensichtlicher
</textarea>
</section>

	<section data-markdown class="fragments">
		<textarea data-template>
### Woher weiß man, dass man ein neues Modell in Produktion braucht?

1. Schon in der Explorationsphase prüfen wie sich das Modell auf neueren Daten verhält
   * wie schnell degradiert die Performance?
   * Mindestens einmal im Jahr, damit man überhaupt noch weiß wie es geht
1. Wenn die Metrik des Modells nachlässt in Produktion
   * Dafür braucht man die Ground Truth der Daten aus Produktion
   * Manchmal bekommt man diese unmittelbar nach der Vorhersage durch die Reaktion eines menschlichen Benutzers
   * Oft aber auch erst nach nennenswerter Verzögerung 
1. *Wenn sich die Verteilung der Daten der Anfragen oder Vorhersagen deutlich von denen des Trainings unterscheiden* 

</textarea>
	</section>

<section data-markdown>
<textarea data-template>
### Verteilung?

* die wichtigste: Normalverteilung aka Gauß-Verteilung
* Histogramme (Binning)

<img src="img/nobel.svg">

</textarea>
</section>


<section data-markdown class="fragments">
  <textarea data-template>
### Verteilungen

<img src="img/causal-insurance/age-reference.png">

Das Alter der Versicherten zum Zeitpunkt des Trainings
</textarea>
</section>


<section data-markdown class="fragments">
<textarea data-template>
### Zum Zeitpunkt des Trainings?

<img src="img/drift_entschlackt.png" style="height: 100%;">

</textarea>
</section>

<!-- <section data-markdown class="fragments">
<textarea data-template>
### So sieht die Verteilung jetzt aus

<img src="img/causal-insurance/age-current.png">

</textarea>
</section>
-->
<section data-markdown class="fragments">
<textarea data-template>
### Driftet das?

<img src="img/causal-insurance/age_no_drift_p75_no_clue.png">

</textarea>
</section>

<section data-markdown class="fragments">
<textarea data-template>
### Und das?

<img src="img/causal-insurance/age_drift_p0_no_clue.png">

</textarea>
</section>

<section data-markdown class="fragments">
<textarea data-template>
### Wie erkennen wir Drift?

* Es wird ein statistischer Test auf den Verteilungen ausgeführt
* Die Anfragen in Produktion werden verglichen mit dem Referenz-Datensatz, den wir zum Training benutzt haben

</textarea>
</section>



<section data-markdown>
	<textarea data-template>
### Monitoring mit Evidently, Prometheus und Grafana

<img src="img/mlops/evidently_grafana_service.png">

<small>https://evidentlyai.com/blog/evidently-and-grafana-ml-monitoring-live-dashboards
<br>
https://docs.evidentlyai.com/integrations/evidently-and-grafana
</small>

</textarea>
</section>

<section data-markdown>
### Wir setzen einen eigenen Monitoring-Server auf

* Basiert auf Prometheus, Grafana und Evidently
* Code basiert auf https://github.com/evidentlyai/evidently/tree/main/examples/integrations/grafana_monitoring_service
* Nicht direkt Teil des eigentlichen Prod-Servers
* Zusätzliche Requests werden gegen den Monitoring-Server gemacht
* Requests über die Zeit verteilt werden simuliert
</section>
	
<section data-markdown>
### Prometheus

Metric Server

* https://prometheus.io/
* https://prometheus.io/docs/prometheus/latest/getting_started/
  * https://prometheus.io/download/

</section>

<section data-markdown>
### Grafana

Visualisierung mit Dashboards
	
https://play.grafana.org/
</section>

<section data-markdown>
### Evidently

ML Performance Monitoring
	
https://evidentlyai.com/
</section>

<section data-markdown>
	<textarea data-template>
### Drift-Erkennung mit Prometheus, Evidently und Grafana

<img src="img/mlops/grafana-evidently-drift.png">

https://docs.evidentlyai.com/reports/data-drift
</textarea>
</section>

<section data-markdown class="fragments">
	<textarea data-template>
### Wie erkennen wir Drift?

* Es wird ein Statistischer Test auf den Eingabe-Daten ausgeführt
* Unsere Features sind numerisch und kategorisch
  * in `metrics_app/config.yaml` festgelegt
* Die Anfragen in Production werden verglichen mit Referenz-Datensatz, den wir zum Training benutzt haben (`datasets/insurance`)
* Evidently sucht als Default eine passende Metrik aus, es muss also nicht unser Problem sein
* Man kann aber auch von Hand konfigurieren, sowohl Test als auch Parameter
  * https://docs.evidentlyai.com/user-guide/customization/options-for-statistical-tests
  * https://docs.evidentlyai.com/user-guide/customization/options-for-data-target-drift

https://docs.evidentlyai.com/reports/data-drift

</textarea>
</section>

<section data-markdown>
	<textarea data-template>
## Welcher Statistischer Test / welche Metrik?

es gibt leider nicht den einen passenden Test

* manche passen nur gut für kleine (< 1000) Datenmengen
  * unsere Datenmengen sind größer als 1000
* manche können nicht nur auf numerischen sondern kategorischen Daten arbeiten
  * wir brauchen beides
* manche sind zwischen 0 und 1 normiert
  * das ist uns eher egal
* unsere Metriken
  * Wasserstein Metrik für numerische Daten
  * Jensen-Shannon Distanz für kategorische Daten  

https://evidentlyai.com/blog/data-drift-detection-large-datasets
</textarea>
</section>

<section data-markdown>
	<textarea data-template>
### Die Wasserstein-Metrik

_Wenn jede Verteilung als ein Haufen von „Erde“ angehäuft auf dem metrischen Raum betrachtet wird, dann beschreibt diese
Metrik die minimalen „Kosten“ der Umwandlung eines Haufens in den anderen._

* nicht zu sensitiv, zeigt nur größere Veränderungen an
* normiert in Veränderungen in Standardabweichungen
* kann (offensichtlich) über 1 gehen
* ab 0.1 gehen wir von einem Drift aus
* funktioniert nur für numerische Daten

https://de.wikipedia.org/wiki/Wasserstein-Metrik
</textarea>
</section>

<section data-markdown>
	<textarea data-template>
### Jensen-Shannon Divergenz

* Jensen-Shannon Distanz ist wie Wurzel aus der Divergenz, das ist unsere Metrik
* zwischen 0 und 1
* ab 0.1 gehen wir von einem Drift aus
* funktioniert auch für kategorische Daten
* basiert auf Kullback–Leibler Divergenz, relative Entropie
* Histogramme werden verglichen, Größe des Samples daher egal
* Binning für kategorische Daten offensichtlich
* Intuition: wie viel Information/Entropie/Überraschung steckt im Unterschied der beiden Verteilungen?


https://en.wikipedia.org/wiki/Jensen%E2%80%93Shannon_divergence
https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence
</textarea>
</section>

<section data-markdown>
<textarea data-template>
### Ergebnisse der Tests der beiden potentiellen Drifts

<div class="container">
<div class="col">
<img src="img/causal-insurance/age_no_drift_p75.png" style="height: 100%;">
<em>Kein Drift</em>
</div>
<div class="col">
<img src="img/causal-insurance/age_drift_p0.png" style="height: 100%;">
<em>Drift</em>

</div>
</div>

</textarea>
</section>


	<section data-markdown class="hands-on">
		<textarea data-template>
## Hands-On IV - Monitoring und Sinn daraus machen

### Teil I: Services checken 

* Monitoring App, Metrics aus Evidently: http://localhost:8085/metrics
* Prometheus: http://localhost:9090
* Grafana: http://localhost:3000

</textarea>
	</section>

  <section data-markdown>
    <textarea data-template>
### Unsere Zeitreise beginnt
</textarea>
</section>

<section data-markdown>
  <textarea data-template>
### Produktion simulieren

* wir simulieren 3 Jahre Betrieb
* jeder Monat hat 1500 Datensätze
* insgesamt 36 Monate, 54.000 Datensätze
  </textarea>
</section>

	<section data-markdown class="hands-on">
		<textarea data-template>
## Hands-On IV - Monitoring und Sinn daraus machen

### Teil II: Request simulieren

1. Einloggen in Grafana: http://localhost:3000
   1. admin/admin
1. Öffne ein Terminal aus deinem Notebook Server heraus (New->Terminal) oder nutze dein bestehendes
1. Mit `./scripts/example_run_request.py -e evidently_service` Anfragen simulieren
1. Das Dashboard `Evidently Data Drift Dashboard` aufrufen
1. Darin den Datensatz `insurance` auswählen (dies kann eine Minute dauern)
1. Stelle sicher, dass sich das Dashboard aktualisiert
1. Was beobachtest du? Welche Features driften? Wie kann man das erklären?

</textarea>
	</section>

  <section data-markdown>
  <textarea data-template>
### Manche Features driften

<img src="img/feature-drift.png">
  </textarea>
</section>

<section data-markdown>
  <textarea data-template>
### Was ist passiert?

* die Performance des Modells degradiert wahrscheinlich
* aber wir haben erst nach Jahren eine Ground Truth, die uns das anhand der Metrik zeigt
* als Ersatzmetrik nehmen wir Feature drift
  * miles: extremer Drift
  * age: signifikanter Drift
  * emergency_braking: signifikanter Drift

  </textarea>
</section>

<section data-markdown>
## Alarm bei Data Drift

* Verändert sich die Art der Eingabedaten?
  * Oft ausgedrückt durch die Verteilung der einzelnen Features

* Muss nicht notwendig schlimm sein
  * Wenn entscheidende Features betroffen sind, kann es ein Problem sein 
* Alarm anhand von
  * Wichtigkeit der driftenden Features
  * Anzahl der driftenden Features
  * Ausmaß des Drifts
<!-- * Dafür wichtig
  * Vernünftige Tests anhand der Metriken
  * Vernünftige Konfidenzintervalle -->

</section>

<section data-markdown style="font-size: x-large;">
	<textarea data-template>
## Drift erfordert Interpretation

Wenn die Welt sich ändert, ist Drift zu erwarten und damit ok

|   | Positive Interpretation, keine Maßnahme erforderlich  | Negative Interpretation, Maßnahme erforderlich  |
|---|---|---|
| *Data und Prediction Drift*  | wichtige Features haben sich geändert, Modell kommt klar und extrapoliert gut, z.B.: Höheres Alter, mehr Risiko  |  wichtige Features haben sich geändert, Modell extrapoliert nicht sinnvoll |
| *Data aber kein Prediction Drift*  | keine wichtigen Features geändert, das Modell ist robust genug für den Drift  | wichtige Features geändert, Modell extrapoliert nicht sinnvoll |
| *Prediction aber kein Data Drift*  | ???  | wahrscheinlich Concept Drift, neue Analyse der Situation notwendig |
|   |   |   |

</textarea>
</section>

<section data-markdown>
### Was ist hier passiert?
  
1. Leute werden immer Älter, das passiert aber langsam (age)
1. Es wird immer weniger Auto gefahren, Leute steigen um auf die Bahn und öffentliche Verkehrsmittel (miles)
1. Die Sicherheit der Autos wird immer besser und der Einfluss der individuellen Fahrleistung wird verringert (emergency_braking, pred) 
 
</textarea>
</section>


    <section data-markdown>
  <textarea data-template>
### Drift über die Zeit

<img src="img/tw-drift-3.png">
  </textarea>
</section>

<section data-markdown>
## Maßnahmen bei Drift

* *Neue Version des Modells trainieren*
  * Neue Daten aufnehmen (und labeln)
  * Neue Features erzeugen
* Schnelle Maßnahme
  * Pre-/Post-Processing des Modells neu kalibrieren
  * Schwellwerte für Anwendung anpassen
  * Bestimmte Bereiche ausklammern 
  * Modell Architektur ändern (oder fixen) und neu trainieren
* Sehr schnelle Maßnahme: Fallback
  * Manuell
  * Heuristik / Baseline
</section>

<section data-markdown class="fragments">
	<textarea data-template>
### Wir gehen zurück in die Phasen I und II

* hier wird wieder in Notebooks gearbeitet
* die Bibliotheken werden inkludiert und bei jeder Änderung neu geladen
* Jupyter Lab bietet eine gemeine Oberfläche für Notebooks und Bibliotheken
* Eine Kombination von Visual Studio Code und Jupyer Notebooks ist ebenso möglich
* Rückkehr in Phase I muss nicht radikal sein
  * wenn Ansatz vergleichbar kann Phase II so erhalten bleiben
  * neue Ergebnisse fliesen dann iterativ in die Professionalisierung ein
</textarea>
</section>

<section data-markdown class="fragments">
	<textarea data-template>
### Rückkehr in Phase II: Wie trainiert man neu/nach/weiter?

<img src="img/mlops/stateful-vs-stateless-v2.png">

</textarea>
</section>


<section data-markdown>
	<textarea data-template>
## Agenda

1. Einleitung / Grundlagen
2. Professionalisierung
3. Betrieb
4. Monitoring
5. *Abschluss*
	</textarea>
</section>

<section data-markdown class="todo">
<textarea data-template>
### Zusammenfassung

1. Machine Learning Projekte können in Phasen gedacht werden
1. In der ersten Phase macht man möglichst schnelle Experimente
1. Sollte sich eine Idee als tragfähig erweisen, professionalisiert man die Idee
1. Dies ist Voraussetzung und Grundlage für Produktion
1. In Produktion ergeben sich besondere Herausforderung im Bereich Monitoring
1. Typischerweise müssen Machine Learning Systeme regelmäßig nachtrainiert und gepflegt werden
</textarea>
</section>

<section data-markdown>
<textarea data-template>
## Zeit für offene Fragen

</textarea>
</section>

<section data-markdown class="fragments">
	<textarea data-template>
### Ausblick: Wann liegt Ground Truth vor?

*Menschliche Experten können die Ground Truth*
* *bestimmen* 
  * sobald ein Mensch die Entscheidungen nachprüft / revidiert
  * bei einem Proposal-System kann das sehr schnell sein
  * bei Dunkelverarbeitung sollte dies in regelmäßigen Abständen passieren
* *nicht bestimmen*
  * wir müssen Realitäten abwarten
  * bei Zeitreihen wird auf die nahe Zukunft vorhergesagt, sobald diese Eintritt kann überprüft werden
  * oft sind solche Realitäten erst nach einiger Zeit wahrnehmbar und unterliegen statistischen Schwankungen (wie bei uns)
</textarea>
</section>

<section data-markdown class="fragments">
	<textarea data-template>
### Ausblick: Unsere Vorhersage selbst erzeugt Drift

* Ein Deployment ändert das unsere Rolle von Beobachter zu Akteur
* Wir versichern nur Leute mit einer guten Risiko Prognose
  * Wenn nicht, warum sollten wir dann eine überhaupt eine Prognose machen?
* Unsere GT wird mehr und mehr gute Fahrer haben
  * Zumindest ist das unsere Hoffnung (sonst hätte die Prognose nicht geklappt)
* Falls nicht (False Negative, Type II Fehler)
  * haben Menschen gelernt, unser System auszutricksen?
  * "Dann versichert eben meine Tochter den Wagen"
* Haben wir gute Fahrer aus verstehen nicht versichert (False Positive, Type I Fehler)
  * Möglichkeit: *epsilon-greedy* meistens der Vorhersage glauben, aber manchmal (epsilon) auch einen Fahrer mit schlechter Prognose versichern 
  * Vorhersage mit allen Wahrscheinlichkeiten geht in unsere Datenbank ein

https://twitter.com/ChristophMolnar/status/1569644089724764160
</textarea>
</section>

		<section data-markdown>
			<textarea data-template>
# Vielen Dank

## MLOps
### Wie bringt man ein ML-Modell in Produktion und hält es dort?

Bleibt gern im Kontakt

Tobias Kurzydym / tobias.kurzydym@openknowledge.de
https://twitter.com/tkurzydym

Yannick Habecker / yannick.habecker@openknowledge.de

Oliver Zeigermann / oliver@zeigermann.de
https://www.linkedin.com/in/oliver-zeigermann-34989773/
https://twitter.com/DJCordhose

</textarea>
		</section>

		</div>
	</div>
	<script src="revealjs/reveal.js/dist/reveal.js"></script>
	<script src="revealjs/reveal.js/plugin/notes/notes.js"></script>
	<script src="revealjs/reveal.js/plugin/markdown/markdown.js"></script>
	<script src="revealjs/reveal.js/plugin/highlight/highlight.js"></script>
	<script src="revealjs/config.js"></script>


</body>

</html>